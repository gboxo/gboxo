<!DOCTYPE html>
<html>
<head>
<title>2023-09-22-clustering-embeddings-part1.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#definition">Definition</a></li>
<li><a href="#usage">Usage</a></li>
</ul>
</li>
<li><a href="#what-are-word-embeddings">What are word embeddings?</a>
<ul>
<li><a href="#different-word-embeddings">Different word embeddings</a></li>
<li><a href="#semantic-similarity">Semantic similarity</a></li>
</ul>
</li>
<li><a href="#clustering">Clustering</a>
<ul>
<li><a href="#clustering-techniques">Clustering techniques</a></li>
<li><a href="#brief-explanation-of-the-technique-family-and-its-interpretation-when-clustering-word-embeddings">Brief explanation of the technique family and it's interpretation when clustering word embeddings.</a></li>
<li><a href="#metrics-for-clusterbility">Metrics for clusterbility</a></li>
<li><a href="#examples-of-interpetable-clusters">Examples of interpetable clusters</a></li>
</ul>
</li>
<li><a href="#distribution-of-word-embeddings">Distribution of word embeddings</a>
<ul>
<li><a href="#how-much-do-similar-tokens-share-representations">How much do similar tokens share representations</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="introduction">Introduction</h2>
<h3 id="definition">Definition</h3>
<p><strong>Word embeddings</strong> are mathematical representations of words, encapsulating their meanings, syntactic attributes, and contextual relations.</p>
<p><strong>Embedding space</strong>
The embadding space is formed by a large collection of so called embedding vectors.</p>
<h3 id="usage">Usage</h3>
<p>Word embeddings are extensively used in various Natural Language Processing (NLP) tasks such as sentiment analysis, machine translation, and text summarization.</p>
<h2 id="what-are-word-embeddings">What are word embeddings?</h2>
<p>Word embeddings are often the result of training some language model on a corpus.
The main characteristics of an embedding space are:</p>
<ul>
<li>Their dimesionality:</li>
</ul>
<p>This is the length of a single word embedding and can go from 50 vector entries in small models to the 1000s in the largest models.
Such a high dimesionality is key to encode certain relationships on embedding space as result of minimizing some loss function.</p>
<ul>
<li>The vocabulary space:</li>
</ul>
<p>This is; the number of different vectors thtat define the embedding space.
Most of the time this number is set in advance when pretraining the model and is largley decided by the tokenizer.
The tokenizer is an algorithm that chunks the corpus in smaller units,in order to get a rich representation but with a limited number of possible tokens.</p>
<p>Usually the vocabulary is in the 10s of thousands of words.
Arounnd 50000 for the  GPT familty and 30000 for BERT.</p>
<h3 id="different-word-embeddings">Different word embeddings</h3>
<p>There exist differnt word embeddings due to several factors such as:</p>
<ul>
<li>Tokenizer</li>
<li>Vocabulary size</li>
<li>Embedding size</li>
<li>Model structure</li>
<li>Corpus pretrained on</li>
<li>Random seeds</li>
<li>etc</li>
</ul>
<p>Some popular models that produce word embeddings are the following:</p>
<ul>
<li>One-Hot Encoding</li>
<li>Word2Vec</li>
<li>GloVe</li>
<li>FastText</li>
<li>BERT</li>
</ul>
<h3 id="semantic-similarity">Semantic similarity</h3>
<p>Work dating back to the 2010's showed that in some cases, embeddings could be operated in a straightworward  manner.
Such in the famous example of:</p>
<p><strong>king-man=queen-woman</strong></p>
<p>In the following years several algorithms and strategies where specially designed to find such semantic pairs, most of the time involving the computing of cosine similarity of embeedings to find the &quot;closest&quot; word that was token at.</p>
<h2 id="clustering">Clustering</h2>
<p>Following past work done on clustering word emnbedding and given the fact that some ammount of semantic relations can be found between word embeddings, the question I want to ask is If we cluster the embedding space do some/any of the produced clusters encode semantic families?</p>
<p>To answer this question we will use BERT uncased trained on a general corpus in english, and several clustering techniques to try finding interpretable clusters.</p>
<h3 id="clustering-techniques">Clustering techniques</h3>
<p>There exist a varity of clustering techniques, a straight-forward way of classifying clustering technques in based ont the nature of the algorithm.
The 3 main kinds of clusterin we are gonna use are:</p>
<ul>
<li>Hierarchical clustering
<ul>
<li>Agglomerative Clustering</li>
</ul>
</li>
<li>Partitioning methods
<ul>
<li>k-means</li>
<li>DBSCAN</li>
</ul>
</li>
<li>Graph Based
<ul>
<li>Louvain method</li>
</ul>
</li>
</ul>
<h3 id="brief-explanation-of-the-technique-family-and-its-interpretation-when-clustering-word-embeddings">Brief explanation of the technique family and it's interpretation when clustering word embeddings.</h3>
<p><strong>Hierarchical Clustering</strong></p>
<p>This family of techniques build tree-like strucutres by conitnously dividing larger clusterd intro smaller oned.
The result is a dendrogram tha we can <code>cut</code> at any level, to get a certain number of clusters.</p>
<p>Applied on embedding space, hierarchical clustering might give us the oprtunity of observing semantic similarities at different levels of granularity.</p>
<p><strong>Partitioning Methods</strong></p>
<p>This familty of techniques divide the space into partitions, provided that we specify in advance the number of clusters we want.</p>
<p>In advance, partitioning methods seem the weaker of the technques given that we msut specify the number of clusters and that the centroid might not well capture the sematnic motif of the cluster.</p>
<p><strong>Graph-Based Methods</strong></p>
<p>In this technique family a grpah is costructed where the datapoints are the nodes, and the similarity between two datapoints is the weight of the edge that connecte the 2 nodes.
The process iteratively preserves or cuts some edges given the objective of findinf clusters or communites.</p>
<p>Graph based methods should be well suited for detecting intrincate relationships between embeddings.</p>
<h3 id="metrics-for-clusterbility">Metrics for clusterbility</h3>
<ul>
<li>Silhouette score</li>
</ul>
<p>Agglomerative</p>
<p><img src="image-13.png" alt=""></p>
<p>KMEANS</p>
<p><img src="image-12.png" alt=""></p>
<h3 id="examples-of-interpetable-clusters">Examples of interpetable clusters</h3>
<p><strong>Agglomerative clustering  10 clusters</strong></p>
<p><img src="image.png" alt="Composed of ordinals"></p>
<p><img src="image-1.png" alt="Composed of non latin characters"></p>
<p><strong>Agglomerative clustering  100 clusters</strong></p>
<p><img src="image-2.png" alt=""></p>
<p><img src="image-3.png" alt=""></p>
<p><strong>k-means 100 clusters</strong></p>
<p><img src="image-4.png" alt="-ing termination"></p>
<p><img src="image-5.png" alt=""></p>
<p><strong>Louvain threshold 0.5 resolution 0.2</strong></p>
<p><img src="image-9.png" alt=""></p>
<p><img src="image-10.png" alt=""></p>
<p><img src="image-11.png" alt=""></p>
<p><code>The effectivness of the clustering is largley influenced by the threshold and the resolution</code></p>
<p><strong>Empirical findings show that selecting the threshold and resolution scoring by:</strong></p>
<p>$$</p>
<p>\text{return} = \text{num_communities} \times \frac{1}{N} \sum_{i=1}^{N} \text{node_counts}_i
$$</p>
<h2 id="distribution-of-word-embeddings">Distribution of word embeddings</h2>
<p>In <em>An Isotropy Analysis in the Multilingual BERT Embedding Space</em> is shown that the embedding space has an anisotropic distribution.
Isotropy is measurec with the mean pairwise cosine similarity aswell as with the product of the fraction of the max PC over the minimum PC times the</p>
<p>Specifically the anisotropic inducing behavior is concentred in a few components.</p>
<h3 id="how-much-do-similar-tokens-share-representations">How much do similar tokens share representations</h3>
<ul>
<li>Drop the tokens that are too short</li>
<li>Compute the pairwise cosine similarity</li>
<li>Compute the pairwise minimum edit distance</li>
<li>For each row group the tokens by minimum edit distance and average the cosine similarity by group to the reference token</li>
<li>Histogram of the means by minimum eddit distance</li>
</ul>
<p><img src="image-7.png" alt=""></p>
<p><img src="image-8.png" alt=""></p>
<p>If we look at the first 3 minimum edit distance we can observe that some tokens have a very high cosine similarity.
This is largly due to plurals, gender cased, etc</p>

</body>
</html>
